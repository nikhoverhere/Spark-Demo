//package com.nikh.spark.streaming;
//
//import org.apache.log4j.Level;
//import org.apache.log4j.Logger;
//import org.apache.spark.SparkConf;
//import org.apache.spark.api.java.JavaSparkContext;
//import org.apache.spark.sql.Dataset;
//import org.apache.spark.sql.Row;
//import org.apache.spark.sql.SparkSession;
//import org.apache.spark.sql.streaming.DataStreamReader;
//
//public class StreamStructuredData {
//
//	public static void main(String[] args) {
////		// TODO Auto-generated method stub
////		
////		System.setProperty("hadoop.home.dir", "c:/hadoop");
////		Logger.getLogger("org.apache").setLevel(Level.WARN);
////
////		//SparkConf conf = new SparkConf().setAppName("startingSpark").setMaster("local[*]");
////		SparkSession session = SparkSession.builder().master("local[*]").appName("StructuredStreaming")
////								.getOrCreate();
////		
////		Dataset<Row> df = session.readStream().format("kafka").option("kafka.bootstrap.servers", "localhost:9092")
////								.option("subscribe", "viewrecords");
////		df.
////		
//
//
////	}
//
//}
